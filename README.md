# Revenue-prediction

В этой работе решается задача  бинарной классификации, а именно строятся алгоритм, определяющие превысит ли средний заработок человека порог $50k. Каждый объект выборки — человек, для которого известны следующие признаки:
* age - возраст, количественный признак;
* workclass - тип работодателя, количественный признак;
* fnlwgt - итоговый вес объекта, количественный признак;
* education - уровень образования, качественный признак;
* education-num - количество лет обучения, количественный признак;
* marital-status - семейное положение, категориальный признак;
* occupation - профессия, категориальный признак;
* relationship - тип семеныйх отношений, категориальный признак;
* race - раса, категориальный признак;
* sex - пол, количественный признак;
* capital-gain - прирост капитала, количественный признак;
* capital-loss - потери капитала, количественный признак;
* hours-per-week - количество часов работы в неделю, количественный признак

Более подробно про данные можно прочитать [здесь](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names).Целевой признак записан
в переменной *>=50K, <=50K*
***
В этой задаче будут использоваться два алгоритма машинного обучения:
* DecisionTree;
* Random Forest.
***

## Выводы по работе: 
* Алгоритм (**DT**) даёт меньшую точность, чем случайный лес(**RF**) на тестовых данных, а также тренировочных данных при кросс-валидации;
* Алгоритм решающее дерево(**DT**) работает гораздо быстрее, чем случайный лес(**RF**);
* С помощью t-критерия Стьюдента было установлено, что средние значения точности по кросс-валидации у данных моделей статистически значимо отличаются.У модели RF это значение больше. Количество фолдов для кросс-валидационной оценки было увеличено до 20 для большей репрезентативности выборки;
* В целом мы спокойно можем использовать Decision Tree, так как он дал почти такую же точность как Random Forest. При этом понадобилось настроить только один гиперпараметр. RandomForest практически сразу дал очень хороший результат, но подбор гиперпараметров занимает много времени. Поэтому если точность в 1% для нас не так важна, то предпочтительнее будет алгоритм DecisionTree
